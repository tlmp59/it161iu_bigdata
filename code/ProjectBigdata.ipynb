{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65bbd4f-d493-4981-bfea-73e4e1365032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, lit, date_add, explode\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8382e96a-ccb3-4b59-8c5d-2e70b82f8ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project Big Data\") \\\n",
    "    .master(\"spark://192.168.1.16:7077\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1536m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d5ec87-6610-4e8e-98b0-b1fc58b233f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark://192.168.1.16:7077\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.master\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe0fa29-6dc7-4c8a-a654-79099ae0746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to read CSV file: 31.17 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType, DoubleType\n",
    ")\n",
    "\n",
    "# Measure the time taken to read the CSV file for performance evaluation\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the CSV file with optimized options:\n",
    "# - header: the file has a header row\n",
    "# - inferSchema: false (since we have already defined the schema)\n",
    "ps_classification_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://192.168.1.16:9000/data/PS_20174392719_1491204439457_log.csv\")\n",
    "\n",
    "# Increase the number of partitions to optimize parallel processing\n",
    "# Adjust the number of partitions based on the number of cores/workers in your cluster\n",
    "ps_classification_df = ps_classification_df.repartition(24)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Time to read CSV file: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2373b2-4750-4e0a-8c34-b10d50651c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "| 212|TRANSFER|899732.34|C1397711092|          0.0|           0.0|C1158899502|    4052724.59|    4952456.93|      0|             0|\n",
      "| 230|CASH_OUT|103761.13| C231781098|      61707.0|           0.0| C548992066|    1062619.33|    1166380.46|      0|             0|\n",
      "| 230| PAYMENT| 16987.22| C712183135|          0.0|           0.0| M431352104|           0.0|           0.0|      0|             0|\n",
      "| 212| PAYMENT|  5824.95| C829118865|          0.0|           0.0| M953100742|           0.0|           0.0|      0|             0|\n",
      "| 232|CASH_OUT| 79267.42| C519353403|        222.0|           0.0| C576394665|     521021.39|     600288.81|      0|             0|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_classification_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba810eb-e7db-4678-8d75-daace402174e",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5ce15-bfb5-4829-80da-37bb89c03b74",
   "metadata": {},
   "source": [
    "### Schema of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a5bdcf-01ed-4b92-b1ef-3cc70c8fab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_classification_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28ab8ce0-a1bf-4321-b60e-031ebd8e7efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------+-------------+--------------+--------------------+\n",
      "|summary|step     |amount       |oldbalanceOrg|newbalanceOrig|isFraud             |\n",
      "+-------+---------+-------------+-------------+--------------+--------------------+\n",
      "|count  |6362620.0|6362620.0    |6362620.0    |6362620.0     |6362620             |\n",
      "|mean   |243.4    |179861.9     |833883.1     |855113.67     |0.001290820448180152|\n",
      "|stddev |142.33   |603858.23    |2888242.67   |2924048.5     |0.035904796801604175|\n",
      "|min    |1.0      |0.0          |0.0          |0.0           |0                   |\n",
      "|25%    |156.0    |13388.35     |0.0          |0.0           |0                   |\n",
      "|50%    |239.0    |74852.61     |14204.16     |0.0           |0                   |\n",
      "|75%    |335.0    |208695.82    |107303.66    |144207.43     |0                   |\n",
      "|max    |743.0    |9.244551664E7|5.958504037E7|4.958504037E7 |1                   |\n",
      "+-------+---------+-------------+-------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "summary_df = ps_classification_df.summary()\n",
    "summary_df = summary_df.select(\n",
    "    col(\"summary\"),\n",
    "    round(col(\"step\"), 2).alias(\"step\"),\n",
    "    round(col(\"amount\"), 2).alias(\"amount\"),\n",
    "    round(col(\"oldbalanceOrg\"), 2).alias(\"oldbalanceOrg\"),\n",
    "    round(col(\"newbalanceOrig\"), 2).alias(\"newbalanceOrig\"),\n",
    "    col(\"isFraud\")\n",
    ")\n",
    "summary_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779cc225-3f0c-4f7f-b76e-6d634d6ebee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|step|type|amount|nameOrig|oldbalanceOrg|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|   0|   0|     0|       0|            0|             0|       0|             0|             0|      0|             0|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = ps_classification_df.groupBy(ps_classification_df.columns).count().where(\"count > 1\").select(count(\"*\")).collect()[0][0]\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "ps_classification_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in ps_classification_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7249e92b-f729-4a71-91b7-534cde20be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step',\n",
       " 'type',\n",
       " 'amount',\n",
       " 'origin',\n",
       " 'sender_old_balance',\n",
       " 'sender_new_balance',\n",
       " 'destination',\n",
       " 'receiver_old_balance',\n",
       " 'receiver_new_balance',\n",
       " 'isfraud',\n",
       " 'isFlaggedFraud']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_classification_df = (\n",
    "    ps_classification_df.withColumnRenamed('nameOrig', 'origin')\n",
    "       .withColumnRenamed('oldbalanceOrg', 'sender_old_balance')\n",
    "       .withColumnRenamed('newbalanceOrig', 'sender_new_balance')\n",
    "       .withColumnRenamed('nameDest', 'destination')\n",
    "       .withColumnRenamed('oldbalanceDest', 'receiver_old_balance')\n",
    "       .withColumnRenamed('newbalanceDest', 'receiver_new_balance')\n",
    "       .withColumnRenamed('isFraud', 'isfraud')\n",
    ")\n",
    "ps_classification_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0026fb-413c-43a7-a35a-77ea0c2b182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column isFlaggedFraud\n",
    "\n",
    "ps_classification_df = ps_classification_df.drop('isFlaggedFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b17595-15e0-46e0-962e-34e93d296f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+\n",
      "|isfraud|    type|  count|\n",
      "+-------+--------+-------+\n",
      "|      0| CASH_IN|1399284|\n",
      "|      0|CASH_OUT|2233384|\n",
      "|      0|   DEBIT|  41432|\n",
      "|      0| PAYMENT|2151495|\n",
      "|      0|TRANSFER| 528812|\n",
      "|      1|CASH_OUT|   4116|\n",
      "|      1|TRANSFER|   4097|\n",
      "+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_classification_df.groupBy(\"isfraud\", \"type\").count().orderBy(\"isfraud\", \"type\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e873659-a38e-42b9-8051-dde73723ac81",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28efb53-aba4-4f76-b689-3f5b58c24a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+------------------+------------------+-----------+--------------------+--------------------+-------+-----+\n",
      "|step|    type|   amount|     origin|sender_old_balance|sender_new_balance|destination|receiver_old_balance|receiver_new_balance|isfraud|type2|\n",
      "+----+--------+---------+-----------+------------------+------------------+-----------+--------------------+--------------------+-------+-----+\n",
      "| 134|CASH_OUT|139235.45|C1357341371|               0.0|               0.0|C1721599816|           559616.15|            698851.6|      0|   CC|\n",
      "| 133|CASH_OUT|119530.71|C1486137070|               0.0|               0.0| C322662467|          3887841.04|          4082378.64|      0|   CC|\n",
      "| 139| PAYMENT|  8699.19|C1103357114|               0.0|               0.0|M1379888655|                 0.0|                 0.0|      0|   CM|\n",
      "| 138| CASH_IN| 33778.44|C1794878395|           29922.0|          63700.44|C1460662635|                 0.0|                 0.0|      0|   CC|\n",
      "| 137| PAYMENT| 18990.02| C796472522|            1230.0|               0.0| M900551491|                 0.0|                 0.0|      0|   CM|\n",
      "+----+--------+---------+-----------+------------------+------------------+-----------+--------------------+--------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_classification_df = ps_classification_df.withColumn(\n",
    "    \"type2\",\n",
    "    when((col(\"origin\").contains(\"C\")) & (col(\"destination\").contains(\"C\")), \"CC\")\n",
    "    .when((col(\"origin\").contains(\"C\")) & (col(\"destination\").contains(\"M\")), \"CM\")\n",
    "    .when((col(\"origin\").contains(\"M\")) & (col(\"destination\").contains(\"C\")), \"MC\")\n",
    "    .when((col(\"origin\").contains(\"M\")) & (col(\"destination\").contains(\"M\")), \"MM\")\n",
    "    .otherwise(None)\n",
    ")\n",
    "ps_classification_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affda531-681c-4efd-adbb-311150b50316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fraud transactions according to type are below:\n",
      "+-----+-----+\n",
      "|type2|count|\n",
      "+-----+-----+\n",
      "|   CC| 8213|\n",
      "+-----+-----+\n",
      "\n",
      "Number of valid transactions according to type are below:\n",
      "+-----+-------+\n",
      "|type2|  count|\n",
      "+-----+-------+\n",
      "|   CC|4202912|\n",
      "|   CM|2151495|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fraud_trans = ps_classification_df.filter(col(\"isfraud\") == 1)\n",
    "valid_trans = ps_classification_df.filter(col(\"isfraud\") == 0)\n",
    "\n",
    "# Count occurrences of each type2 category for fraud transactions\n",
    "print(\"Number of fraud transactions according to type are below:\")\n",
    "fraud_trans.groupBy(\"type2\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# Count occurrences of each type2 category for valid transactions\n",
    "print(\"Number of valid transactions according to type are below:\")\n",
    "valid_trans.groupBy(\"type2\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "987fef28-bdca-4c1e-8aeb-5102532af068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+------------------+------------------+--------------------+--------------------+-------+-----+\n",
      "|step|    type|   amount|sender_old_balance|sender_new_balance|receiver_old_balance|receiver_new_balance|isfraud|type2|\n",
      "+----+--------+---------+------------------+------------------+--------------------+--------------------+-------+-----+\n",
      "|  45|CASH_OUT| 45370.04|           20220.0|               0.0|            44798.09|            90168.13|      0|   CC|\n",
      "|  43| PAYMENT| 17647.42|             353.0|               0.0|                 0.0|                 0.0|      0|   CM|\n",
      "|  23| CASH_IN|169446.27|         837200.77|        1006647.03|           429690.03|           260243.76|      0|   CC|\n",
      "|  44| PAYMENT| 14549.29|           50641.0|          36091.71|                 0.0|                 0.0|      0|   CM|\n",
      "|  23| PAYMENT|  5780.19|           31373.0|          25592.81|                 0.0|                 0.0|      0|   CM|\n",
      "|  44|TRANSFER|428879.21|            2610.0|               0.0|           144015.82|           572895.03|      0|   CC|\n",
      "|  43| PAYMENT|  2222.45|             107.0|               0.0|                 0.0|                 0.0|      0|   CM|\n",
      "| 122|CASH_OUT|215915.88|               0.0|               0.0|           601923.41|           817839.29|      0|   CC|\n",
      "|  38| PAYMENT|  33804.3|               0.0|               0.0|                 0.0|                 0.0|      0|   CM|\n",
      "|  34| CASH_IN| 60424.93|         853329.02|         913753.95|          5686418.24|          5625993.31|      0|   CC|\n",
      "|  48| PAYMENT|  3247.72|           11036.0|           7788.28|                 0.0|                 0.0|      0|   CM|\n",
      "|  35| PAYMENT| 49111.96|         517140.97|          468029.0|                 0.0|                 0.0|      0|   CM|\n",
      "|  45|TRANSFER| 49482.37|           32878.0|               0.0|           541884.25|           591366.62|      0|   CC|\n",
      "|  21| PAYMENT|  2227.84|           2601.62|            373.78|                 0.0|                 0.0|      0|   CM|\n",
      "|  42| PAYMENT|  5623.32|             771.0|               0.0|                 0.0|                 0.0|      0|   CM|\n",
      "|  36|CASH_OUT| 70861.81|               0.0|               0.0|           180171.22|           251033.03|      0|   CC|\n",
      "|  39|CASH_OUT| 23886.53|               0.0|               0.0|          1025823.84|          1049710.36|      0|   CC|\n",
      "|  93| CASH_IN|140109.92|          93613.16|         233723.08|           2311388.8|          2171278.88|      0|   CC|\n",
      "|  43|CASH_OUT|203793.79|               0.0|               0.0|          2128450.03|          2332243.82|      0|   CC|\n",
      "|  45|CASH_OUT| 32891.12|          49263.95|          16372.84|            83190.32|           116081.43|      0|   CC|\n",
      "+----+--------+---------+------------------+------------------+--------------------+--------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop 'origin' and 'destination' columns\n",
    "ps_classification_df = ps_classification_df.drop(\"origin\", \"destination\")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "ps_classification_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a324426-73f4-4901-aaf6-8b59fb023801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- sender_old_balance: double (nullable = true)\n",
      " |-- sender_new_balance: double (nullable = true)\n",
      " |-- receiver_old_balance: double (nullable = true)\n",
      " |-- receiver_new_balance: double (nullable = true)\n",
      " |-- isfraud: integer (nullable = true)\n",
      " |-- type2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_classification_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f11c0-3369-40c1-94ff-8020c77acbab",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80de7e31-89e4-4841-8910-9f7a2e8347ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"isfraud\", outputCol=\"label\")\n",
    "ps_decision_tree_classifier_df = label_indexer.fit(ps_classification_df).transform(ps_classification_df)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = [\"type\", \"type2\"]\n",
    "numerical_cols = [\"amount\", \"sender_old_balance\", \"sender_new_balance\",\n",
    "                  \"receiver_old_balance\", \"receiver_new_balance\"]\n",
    "\n",
    "# Pipeline stages\n",
    "stages = []\n",
    "\n",
    "# StringIndexer and OneHotEncoder for categorical features\n",
    "for col in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=f\"indexed_{col}\")\n",
    "    encoder = OneHotEncoder(inputCol=f\"indexed_{col}\", outputCol=f\"encoded_{col}\")\n",
    "    stages.extend([indexer, encoder])\n",
    "\n",
    "# Assemble all features into a single vector\n",
    "assembler_inputs = [f\"encoded_{col}\" for col in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages.append(assembler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c199511-e4c5-4446-b37d-4325a421c82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9988\n",
      "Accuracy: 0.9991\n",
      "Precision: 0.9990\n",
      "Recall: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(CrossValidatorModel_b3fa2f38f815,\n",
       " {'F1-Score': 0.9988391896438635,\n",
       "  'Accuracy': 0.9990813561666226,\n",
       "  'Precision': 0.9990189177158473,\n",
       "  'Recall': 0.9990813561666227})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(labelCol=\"isfraud\", featuresCol=\"features\", seed=42)\n",
    "stages.append(dt_classifier)\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Split data into training and test sets (stratified split)\n",
    "training, test = ps_decision_tree_classifier_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Parameter grid for cross-validation (hyperparameter tuning)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt_classifier.maxDepth, [1,2]) \\\n",
    "    .addGrid(dt_classifier.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Train the cross-validated model (F1 as the optimization metric)\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"f1\"),\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(training)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = cv_model.transform(test)\n",
    "\n",
    "# Define evaluators for different metrics\n",
    "evaluators = {\n",
    "    \"F1-Score\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"f1\"),\n",
    "    \"Accuracy\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"accuracy\"),\n",
    "    \"Precision\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"weightedPrecision\"),\n",
    "    \"Recall\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"weightedRecall\")\n",
    "}\n",
    "\n",
    "# Compute and print all evaluation metrics\n",
    "metrics = {metric_name: evaluator.evaluate(predictions) for metric_name, evaluator in evaluators.items()}\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Return trained model and metrics\n",
    "cv_model, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6f4d2-711d-45b6-98ff-5bf106e1e3f5",
   "metadata": {},
   "source": [
    "### Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db79ba44-edca-4ddb-abad-57de492e0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"isfraud\", outputCol=\"label\")\n",
    "ps_logistic_regression_df = label_indexer.fit(ps_classification_df).transform(ps_classification_df)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = [\"type\", \"type2\"]\n",
    "numerical_cols = [\"amount\", \"sender_old_balance\", \"sender_new_balance\",\n",
    "                  \"receiver_old_balance\", \"receiver_new_balance\"]\n",
    "\n",
    "# Pipeline stages\n",
    "stages = []\n",
    "\n",
    "# StringIndexer and OneHotEncoder for categorical features\n",
    "for col in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=f\"indexed_{col}\")\n",
    "    encoder = OneHotEncoder(inputCol=f\"indexed_{col}\", outputCol=f\"encoded_{col}\")\n",
    "    stages.extend([indexer, encoder])\n",
    "\n",
    "# Assemble all features into a single vector\n",
    "assembler_inputs = [f\"encoded_{col}\" for col in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages.append(assembler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e50b774-e589-4b55-964c-288f5f69cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Logistic Regression Metrics:\n",
      "F1-Score: 0.9981\n",
      "Accuracy: 0.9988\n",
      "Precision: 0.9975\n",
      "Recall: 0.9988\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "### Define Logistic Regression model\n",
    "lr_classifier = LogisticRegression(labelCol=\"isfraud\", featuresCol=\"features\", maxIter=100, regParam=0.01)\n",
    "\n",
    "# Add model to the pipeline\n",
    "lr_pipeline = Pipeline(stages=stages + [lr_classifier])\n",
    "\n",
    "# Split data into training and test sets\n",
    "training, test = ps_logistic_regression_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Hyperparameter tuning grid\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr_classifier.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr_classifier.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr_pipeline,\n",
    "    estimatorParamMaps=lr_param_grid,\n",
    "    evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"f1\"),\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# Train model\n",
    "lr_cv_model = lr_cv.fit(training)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_cv_model.transform(test)\n",
    "\n",
    "# Evaluate metrics\n",
    "lr_metrics = {\n",
    "    \"F1-Score\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"f1\").evaluate(lr_predictions),\n",
    "    \"Accuracy\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"accuracy\").evaluate(lr_predictions),\n",
    "    \"Precision\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"weightedPrecision\").evaluate(lr_predictions),\n",
    "    \"Recall\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"weightedRecall\").evaluate(lr_predictions),\n",
    "}\n",
    "# Print metrics\n",
    "print(\"\\nüîç Logistic Regression Metrics:\")\n",
    "for metric, value in lr_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8542c-1a9c-41c1-a58f-7aec1d83072b",
   "metadata": {},
   "source": [
    "### Na√Øve Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a382b58-95e4-4d97-ae6c-ea8cd69c11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"isfraud\", outputCol=\"label\")\n",
    "ps_naive_bayes_model_df = label_indexer.fit(ps_classification_df).transform(ps_classification_df)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = [\"type\", \"type2\"]\n",
    "numerical_cols = [\"amount\", \"sender_old_balance\", \"sender_new_balance\",\n",
    "                  \"receiver_old_balance\", \"receiver_new_balance\"]\n",
    "\n",
    "# Pipeline stages\n",
    "stages = []\n",
    "\n",
    "# StringIndexer and OneHotEncoder for categorical features\n",
    "for col in categorical_cols:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=f\"indexed_{col}\")\n",
    "    encoder = OneHotEncoder(inputCol=f\"indexed_{col}\", outputCol=f\"encoded_{col}\")\n",
    "    stages.extend([indexer, encoder])\n",
    "\n",
    "# Assemble all features into a single vector\n",
    "assembler_inputs = [f\"encoded_{col}\" for col in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages.append(assembler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "951f61b2-92bc-4b12-ad36-0d2df152a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Na√Øve Bayes Metrics:\n",
      "F1-Score: 0.9525\n",
      "Accuracy: 0.9115\n",
      "Precision: 0.9983\n",
      "Recall: 0.9115\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define Na√Øve Bayes model\n",
    "nb_classifier = NaiveBayes(labelCol=\"isfraud\", featuresCol=\"features\", modelType=\"multinomial\")\n",
    "\n",
    "# Add model to the pipeline\n",
    "nb_pipeline = Pipeline(stages=stages + [nb_classifier])\n",
    "\n",
    "# Split data into training and test sets\n",
    "training, test = ps_naive_bayes_model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Hyperparameter tuning grid\n",
    "nb_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(nb_classifier.smoothing, [0.0, 1.0, 10.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation\n",
    "nb_cv = CrossValidator(\n",
    "    estimator=nb_pipeline,\n",
    "    estimatorParamMaps=nb_param_grid,\n",
    "    evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"f1\"),\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# Train model\n",
    "nb_cv_model = nb_cv.fit(training)\n",
    "\n",
    "# Make predictions\n",
    "nb_predictions = nb_cv_model.transform(test)\n",
    "\n",
    "# Evaluate metrics\n",
    "nb_metrics = {\n",
    "    \"F1-Score\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"f1\").evaluate(nb_predictions),\n",
    "    \"Accuracy\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"accuracy\").evaluate(nb_predictions),\n",
    "    \"Precision\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"weightedPrecision\").evaluate(nb_predictions),\n",
    "    \"Recall\": MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"isfraud\", metricName=\"weightedRecall\").evaluate(nb_predictions),\n",
    "}\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nüîç Na√Øve Bayes Metrics:\")\n",
    "for metric, value in nb_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d968d274-7a3d-4ff5-8641-e96cf284f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
